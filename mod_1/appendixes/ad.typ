#import "../../common-typst/defs.typ": *

#show: common_styles

#appendix(label: "auto-diff")[
    Automatic Differentiation (AD)
][
    AD provides a way to compute the derivative of a function by using the chain rule without explicitly computing finite differences which would require us to choose a finite difference step size.

    AD is implemented using the #link("https://github.com/LucaCiucci/differential-rs")[`differential`] #footnote[https://github.com/LucaCiucci/differential-rs] package I'm currently developing where `Differential` is just a product type of the function and its derivative. By defining all the basic operations, we can compute the derivative of a large set of functions.

    Suppose, for example, that we want to compute the derivative in $x = 3$ of the function $f(x) = x^2 + sin(x)$. We could define:
    ```rs
    fn f(x: Differential) -> Differential {
        x * x + x.sin()
    }
    ```
    and then do the following:
    + we want the derivative with respect to $x$, the derivative of $x$ with respect to itself is $1$;
    + hence define ```rs x = Differential::new(3.0, 1.0)```
    + compute `f(x)` and extract the derivative.
    This translates into: ```rs f((3.0, 1.0).into()).derivative```

    == Convergence of AD in recursive algorithms <ad-convergence-in-recursive-algorithms>

    Some algorithms might not be a closed form and, instead, rely on recursion. For an example, we might want to apply AD to the result of the Newton algorithm to find roots. In this case we have:
    $
    bold(x)_(n+1) = bold(x)_n - J^(-1)(bold(x)_n) space f(bold(x)_n)
    $ <newton-ad-application-example>
    An example of this application is @TODO[?].

    Another example of a recursive algorithm is described in @ad-1d-taylor-series.

    In practice, we find that, for _well behaved_ recursive algorithms, AD works as expected.\
    I asked myself why this works and whether its usage is justifiable or not.

    Intuitively, it might look obvious that AD works for recursive algorithms because we are just taking the Taylor expansion of the function and, be it recursive or not, the Taylor expansion should always provide us a valid approximation of the function.\
    On the other hand, one might think that, while the central value converges, its is not guaranteed that the Taylor expansion converges (and if so, to what). In other words, we want to be sure that AD does not produce any "sawtooth" pattern.

    Before going on with implementation, we want to make sure that AD works for recursive algorithms that might emerge both in the implementation or in the usage of AD.

    #warning(title: [Meaning _well behaved_ recursive algorithms])[
        I don't have a proper definition for this, but clearly there are algorithms where AD is meaningless.\
        For a counter example, we can consider the Metropolis-Hastings algorithm (@the-metropolis-hastings-algorithm[?]).
        We can study the last point generated by the algorithm as a function of the initial point (i.e. $x_N = f_N (x_0)$).
        We expect that, for a finite number of steps and by fixing the PRNG's seed, by changing just a little bit the initial point, the accepted/rejected steps will be the same and, as a consequence:
        $
        f_N (x_0 + delta) = f_N (x_0) + delta space "for" delta "small"
        $
        This means that AD will always produce the following result:
        $
        forall N: cases(
            x_N = f_N (x_0),
            x'_N = x'_0,
            x''_N = x''_0,
            ...
        )
        $
        This looks pretty boring and strange: we expect that, by changing even by little the initial point, the last point generated by the algorithm will change a lot.\
        The truth is that AD is giving us the correct result but, by increasing $N$, the range of validity of the taylor expansion of $f_N$ around $x_0$ decreases and should converge to $0$ as $N$ goes to infinity.\
        This tels us that taking derivatives of the MH algorithm with respect to the initial condition is meaningless. This was obvious but it is just an example of a recursive algorithm where the application of AD is meaningless.
    ]

    What all these algorithms have in common is that they can be expressed as a recursive succession.\
    Let's consider the parameter-dependant recursive succession:
    $
    a_(n + 1) = f(a_n, p)
    $ <recursive-succession>

    #note[
        We consider only the cases where $f$ is differentiable with respect to $a$ and $p$.
    ]

    We have:
    $
    frac(d, d p) a_(n+1) &= frac(d, d p) f(a_n, p) \
    &= f_x (a_n, p) frac(d, d p) a_n + f_y (a_n, p) \
    $ <recursive-succession-derivative>
    where $f_x = frac(diff, diff x) f(x,y)$ and $f_y = frac(diff, diff y) f(x,y)$ are the partial derivatives of $f$.

    If $a_n$ converges to $a$, the equation becomes:
    $
    frac(d, d p) a = f_x (a, p) frac(d, d p) a + f_y (a, p)
    $
    hence:
    $
    frac(d, d p) a = frac(f_y (a, p), 1 - f_x (a, p))
    $

    The good news is that the fixed point is only one and it is the correct derivative. This equation would also allow us to compute the derivative of the fixed point with respect to $p$ without having to fully compute the succession using AD.

    #note[
        This gives us an optimization chance: we could compute the algorithm using AD and just compute the derivatives of $f$ once we have the fixed point.\
        The reason I would never do this is that, in most cases, the benefit is just not worth it compared to the added complexity (especially in the $N$-D case).
    ]

    I now ask myself:
    + under what conditions the succession converges?
    + that happens if $f_x (a, p) = 1$?

    Answering these questions is trivial. We are interested into looking ad well behaved functions that are $C^1$ in a neighborhood of the fixed point $x_0$.\
    If the succession @recursive-succession converges, we have $abs(f'(x_0)) < 1$, hence the derivatives succession @recursive-succession-derivative also converges to the correct value because of the stability criterion of the fixed point.

    The edge case $f_x (a, p) = 1$ is the case where it is not possible to determine the stability of the fixed point using the first derivative criterion and we would have to investigate higher order derivatives, but this is not the case we are interested in.

    Using induction, we can prove that, if $f$ is smooth, higher order derivatives of the succession also converge to the correct value. This explains why the `sqrt`??? algorithm presented in @ad-1d-taylor-series works.

    == Possible implementations

    There are many implementations of AD. The most common ones are:
    + forward accumulation
    + reverse accumulation

    I prefer forward mode. Reverse mode is often said to be more efficient when working with a large number of input variables as it does not requires to sweep the computational graph for each input variable.\
    I think this is bullshit as we can always use the forward mode by just saying that the `Differential` is a product between the function and its Jacobian (i.e. $(f, bold(J)_f)$) instead of a single derivative. This also does not require to store the computational graph for later exploration as the hole Jacobian is computed at once.

    === 1D Taylor series <ad-1d-taylor-series>

    This was my old implementation #footnote[https://github.com/LucaCiucci/LC/blob/master/include/LC/math/algebra/Differential/Differential] that only accounts for single variable derivatives. In this implementation the `Differential` is just the arbitrary order 1D Taylor expansion of the function.\
    Many basic operations are defined in terms of the Taylor expansion.

    The implementation has some problems as it is not always easy to write the expansion of every operation and, some operations have polynomial truncation edge cases to consider.\
    In particular, it is worth mentioning that, in this particular implementation, I had no explicit algorithm for the square root expansion (because I was lazy) so I backed up to the recursive Erone's algorithm#footnote[https://github.com/LucaCiucci/LC/blob/master/include/LC/math/common/basic.inl#L32] that converges pretty fast for any derivative. A-posteriori, this is justified by @ad-convergence-in-recursive-algorithms.

    === First-order AD <ad-first-order-ad>

    #todo[
        reference, porta qu√¨ quello che sta all'inizio
    ]
    This looks like a downgrade compared to the first implementation as it computes just the first derivative.\
    The advantage is that now, instead of having the 1D Taylor expansion, we can compute hole Jacobians as described above (recall $(f, bold(J)_f)$).

    === Many-order multi-variable AD
    Define the differential:
    #let differential(f) = $D lr(angle.l #f angle.r)$
    #let jac(f) = $bold(J) lr(angle.l #f angle.r)$
    $
    differential(cal(F)) = (cal(F), jac(cal(F)))
    $ <ad-differentials-composition>
    where $cal(F)$ is a field and $jac(cal(F))$ is a jacobian object over the space of $cal(F)$. This notation is chosen \
    Then, a first order differential like described in @ad-first-order-ad would just be $D(RR)$.
    We could then recur and define a second order differential as $D(D(RR))$ and so on.

    As a practical example, let's analyze what $D(D(RR))$ is:
    $
    differential(differential(RR)) = (differential(RR), jac(differential(RR))) = ((RR, jac(RR)), (jac(RR), jac(jac(RR))))
    $
    This looks particularly bad. If we want to take the derivative with respect to two variables $x$ adn $y$, a differential would look like:
    $
    ((f, f_y), (f_x, f_(x y)))
    $
    but we have:
    + 1x $0$-order derivative (OK)
    + 2x $1$-order derivatives (OK)
    + 1x $2$-order derivative (not OK)
    Also, if we want the second order with respect to the same variable, we would get:
    $
    ((f, f_x), (f_x, f_(x x)))
    $ <ad-differential-composition-repetition-problem>
    which is sub-optimal as the first order derivatives are computed and stored twice. If consider the $k$-order, we would get $2^k$ elements to store and compute: this would be problematic as we would usually store elements on the stack, hence we would have to limit the order of the differential to a small number. This is much worse than @ad-1d-taylor-series!

    If we want to recover the advantages of @ad-1d-taylor-series in terms of memory, we would like to rewrite the terms as:
    $
    (f, underbracket(diff_alpha f, #[jacobian]), underbracket(diff_alpha^2 f, #[hessian]), diff_alpha^3 f, ...)
    $ <ad-differential-serialized>
    We notice that, for a single variable, we recover the expression of @ad-1d-taylor-series.

    #todo(title: [future work: reducing memory requirements])[
        For the functions we are usually interested in computing derivatives of, derivatives will commute. For example, for 3 variables and order 3, @ad-differential-serialized would become:
        #let unn(it, ok) = if ok > 0 { text(gray, it) } else { it }

        #text(7pt)[$
        (
            f <-> (0, 0, 0),
            vec(f_x <-> (1, 0, 0), f_y <-> (0, 1, 0), f_x <-> (0, 0, 1)),
            mat(
                vec(unn(f_(x x) <-> (2, 0, 0), #0), unn(f_(x y) <-> (1, 1, 0), #0), unn(f_(x z) <-> (1, 0, 1), #0)),
                vec(unn(f_(y x) <-> (1, 1, 0), #1), unn(f_(y y) <-> (0, 2, 0), #0), unn(f_(y z) <-> (0, 1, 1), #0)),
                vec(unn(f_(z x) <-> (1, 0, 1), #1), unn(f_(z y) <-> (0, 1, 1), #1), unn(f_(z z) <-> (0, 0, 2), #0)),
            ),
            mat(
                cases(
                    vec(unn(f_(x x x) <-> (3, 0, 0), #0), unn(f_(x x y) <-> (2, 1, 0), #0), unn(f_(x x z) <-> (2, 0, 1), #0)),
                    vec(unn(f_(x y x) <-> (2, 1, 0), #1), unn(f_(x y y) <-> (1, 2, 0), #0), unn(f_(x y z) <-> (1, 1, 1), #0)),
                    vec(unn(f_(x z x) <-> (2, 0, 1), #1), unn(f_(x z y) <-> (1, 1, 1), #1), unn(f_(x z z) <-> (1, 0, 2), #0)),
                ),
                cases(
                    vec(unn(f_(y x x) <-> (2, 1, 0), #1), unn(f_(y x y) <-> (1, 2, 0), #1), unn(f_(y x z) <-> (1, 1, 1), #1)),
                    vec(unn(f_(y y x) <-> (1, 2, 0), #1), unn(f_(y y y) <-> (0, 3, 0), #0), unn(f_(y y z) <-> (0, 2, 1), #0)),
                    vec(unn(f_(y z x) <-> (1, 1, 1), #1), unn(f_(y z y) <-> (0, 2, 1), #1), unn(f_(y z z) <-> (0, 1, 2), #0)),
                ),
                cases(
                    vec(unn(f_(z x x) <-> (2, 0, 1), #1), unn(f_(z x y) <-> (1, 1, 1), #1), unn(f_(z x z) <-> (1, 0, 2), #1)),
                    vec(unn(f_(z y x) <-> (1, 1, 1), #1), unn(f_(z y y) <-> (0, 2, 1), #1), unn(f_(z y z) <-> (0, 1, 2), #1)),
                    vec(unn(f_(z z x) <-> (1, 0, 2), #1), unn(f_(z z y) <-> (0, 1, 2), #1), unn(f_(z z z) <-> (0, 0, 3), #0)),
                ),
            )
        )
        $]
        #text(7pt)[$
        (
            f <-> (0, 0, 0),
            vec(f_x <-> (1, 0, 0), f_y <-> (0, 1, 0), f_x <-> (0, 0, 1)),
            mat(
                vec(unn(f_(x x) <-> (2, 0, 0), #0), unn(f_(x y) <-> (1, 1, 0), #0), unn(f_(x z) <-> (1, 0, 1), #0)),
                vec(unn(f_(y x) <-> (1, 1, 0), #1), unn(f_(y y) <-> (0, 2, 0), #0), unn(f_(y z) <-> (0, 1, 1), #0)),
                vec(unn(f_(z x) <-> (1, 0, 1), #1), unn(f_(z y) <-> (0, 1, 1), #1), unn(f_(z z) <-> (0, 0, 2), #0)),
            ),
            mat(
                cases(
                    unn(vec(unn(f_(x x x) <-> (3, 0, 0), #0), unn(f_(x x y) <-> (2, 1, 0), #0), unn(f_(x x z) <-> (2, 0, 1), #0)), #0),
                    unn(vec(unn(f_(x y x) <-> (2, 1, 0), #1), unn(f_(x y y) <-> (1, 2, 0), #0), unn(f_(x y z) <-> (1, 1, 1), #0)), #0),
                    unn(vec(unn(f_(x z x) <-> (2, 0, 1), #1), unn(f_(x z y) <-> (1, 1, 1), #1), unn(f_(x z z) <-> (1, 0, 2), #0)), #0),
                ),
                cases(
                    unn(vec(unn(f_(y x x) <-> (2, 1, 0), #0), unn(f_(y x y) <-> (1, 2, 0), #0), unn(f_(y x z) <-> (1, 1, 1), #0)), #1),
                    unn(vec(unn(f_(y y x) <-> (1, 2, 0), #1), unn(f_(y y y) <-> (0, 3, 0), #0), unn(f_(y y z) <-> (0, 2, 1), #0)), #0),
                    unn(vec(unn(f_(y z x) <-> (1, 1, 1), #1), unn(f_(y z y) <-> (0, 2, 1), #1), unn(f_(y z z) <-> (0, 1, 2), #0)), #0),
                ),
                cases(
                    unn(vec(unn(f_(z x x) <-> (2, 0, 1), #0), unn(f_(z x y) <-> (1, 1, 1), #0), unn(f_(z x z) <-> (1, 0, 2), #0)), #1),
                    unn(vec(unn(f_(z y x) <-> (1, 1, 1), #1), unn(f_(z y y) <-> (0, 2, 1), #0), unn(f_(z y z) <-> (0, 1, 2), #0)), #1),
                    unn(vec(unn(f_(z z x) <-> (1, 0, 2), #1), unn(f_(z z y) <-> (0, 1, 2), #1), unn(f_(z z z) <-> (0, 0, 3), #0)), #0),
                ),
            )
        )
        $]
        so, instead of storing and computing $1 + 3 + 9 + 27 = #(1 + 3 + 9 + 27)$ elements, we could store $1 + 3 + 6 + 10 = #(1 + 3 + 6 + 10)$ elements.This would be a huge improvement for higher order and dimension derivatives because, as $N$ ans $K$ increases, the ratio between necessary elements and stored elements decreases (the tensors become more sparse).

        The problem with this approach is practical: as it is easy to store the elements in a plain format, retrieving the index of the element in the matrix is not trivial and would require to compute sums of multinomial coefficients for each access. If $N$ and $K$ are known at compile time, we could work to somehow pre-compute all the necessary indexes, but in the opposite case this would be problematic.

        I therefore postpone this to a future work since I would always use single variable higher-order derivatives in this project and this optimization would have no effect.
    ]

    Using @ad-differential-serialized we would like to avoid writing the explicit taylor series of every operation and, instead, define the operations in terms of the first derivative as described in @ad-first-order-ad.\

    One option is to remember what @ad-differential-composition-repetition-problem taught us and view @ad-differential-serialized in two different ways:
    $
    (underbracket(#$f, diff_alpha f, diff_alpha^2 f, diff_alpha^3 f, ...$, #[value]), diff_alpha^K f)
    <==>
    (f, underbracket(#$diff_alpha f, diff_alpha^2 f, diff_alpha^3 f, ..., diff_alpha^K f$, #[derivative]))
    $
    The practical implementation of this stuff starts to become quite complex since this tuple is composed of different kinds of elements:
    some languages (like C++) allow for variadic templates and this greatly simplifies the implementation when $K$ is known at compile time.
    In the case of Rust we don't have variadic templates by design, so we have to find other ways to represent this tuple in an elegant manner.\
    I don't think there is a definitive solution for this, we would need an unsafe usage of unions here and we might have problems with unaligned access of memory... ????

    #note[
        This particular implementation only accounts for the case where there is only one order $K$. If we would like to account for different order for different variables, we could just backup to differentials composition as described in @ad-differentials-composition.
    ]
]